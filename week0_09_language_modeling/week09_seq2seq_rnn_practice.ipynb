{"cells":[{"cell_type":"markdown","metadata":{"id":"J0ikRVHxeXYK"},"source":["## week10: seq2seq practice\n","### Generating names with recurrent neural networks\n","\n","This time you'll find yourself delving into the heart (and other intestines) of recurrent neural networks on a class of toy problems.\n","\n","Struggle to find a name for the variable? Let's see how you'll come up with a name for your son/daughter. Surely no human has expertize over what is a good child name, so let us train RNN instead;\n","\n","It's dangerous to go alone, take these:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"c1GjhHdQeXYP","executionInfo":{"status":"ok","timestamp":1640115754098,"user_tz":-180,"elapsed":845,"user":{"displayName":"Муса Сафин","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17185596362499426241"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"1hYiW4jDeXYR"},"source":["# Our data\n","The dataset contains ~8k earthling names from different cultures, all in latin transcript.\n","\n","This notebook has been designed so as to allow you to quickly swap names for something similar: deep learning article titles, IKEA furniture, pokemon names, etc."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GuJmmR23eXYS","executionInfo":{"status":"ok","timestamp":1640115775965,"user_tz":-180,"elapsed":633,"user":{"displayName":"Муса Сафин","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17185596362499426241"}},"outputId":"8d1ec723-edd1-4082-eb0b-21c10be3bd9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-12-21 19:42:55--  https://raw.githubusercontent.com/girafe-ai/ml-mipt/21f_made/week0_09_language_modeling/names\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 55868 (55K) [text/plain]\n","Saving to: ‘names’\n","\n","\rnames                 0%[                    ]       0  --.-KB/s               \rnames               100%[===================>]  54.56K  --.-KB/s    in 0.001s  \n","\n","2021-12-21 19:42:55 (51.1 MB/s) - ‘names’ saved [55868/55868]\n","\n"]}],"source":["# Uncomment this cell in Colab\n","\n","! wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/21f_made/week0_09_language_modeling/names -O names"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"va8-92DaeXYS","executionInfo":{"status":"ok","timestamp":1640116137980,"user_tz":-180,"elapsed":241,"user":{"displayName":"Муса Сафин","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17185596362499426241"}}},"outputs":[],"source":["import os\n","start_token = \" \"\n","\n","with open(\"names\") as f:\n","    names = f.read()[:-1].split('\\n')\n","    names = [start_token + line for line in names]"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NXXWVfBzeXYT","executionInfo":{"status":"ok","timestamp":1640116139740,"user_tz":-180,"elapsed":6,"user":{"displayName":"Муса Сафин","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17185596362499426241"}},"outputId":"9ebfa29a-3c19-425c-da69-66fd203d893d"},"outputs":[{"output_type":"stream","name":"stdout","text":["n samples =  7944\n"," Abagael\n"," Claresta\n"," Glory\n"," Liliane\n"," Prissie\n"," Geeta\n"," Giovanne\n"," Piggy\n"]}],"source":["print ('n samples = ',len(names))\n","for x in names[::1000]:\n","    print (x)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"id":"ArYfIZNpeXYV","executionInfo":{"status":"ok","timestamp":1640116144021,"user_tz":-180,"elapsed":828,"user":{"displayName":"Муса Сафин","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17185596362499426241"}},"outputId":"1e62ac3e-54d7-4a33-ffea-34154fb7cb66"},"outputs":[{"output_type":"stream","name":"stdout","text":["max length = 16\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["MAX_LENGTH = max(map(len, names))\n","print(\"max length =\", MAX_LENGTH)\n","\n","plt.title('Sequence length distribution')\n","plt.hist(list(map(len, names)),bins=25);"]},{"cell_type":"markdown","metadata":{"id":"6xVIws_BeXYW"},"source":["# Text processing\n","\n","First we need next to collect a \"vocabulary\" of all unique tokens i.e. unique characters. We can then encode inputs as a sequence of character ids."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xv15_5KteXYW"},"outputs":[],"source":["#all unique characters go here\n","tokens = <all unique characters in the dataset>\n","\n","tokens = list(tokens)\n","\n","num_tokens = len(tokens)\n","print ('num_tokens = ', num_tokens)\n","\n","assert 50 < num_tokens < 60, \"Names should contain within 50 and 60 unique tokens depending on encoding\""]},{"cell_type":"markdown","metadata":{"id":"72K1T4O3eXYX"},"source":["### Convert characters to integers\n","\n","Torch is built for crunching numbers, not strings. \n","To train our neural network, we'll need to replace characters with their indices in tokens list.\n","\n","Let's compose a dictionary that does this mapping."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LPioELAeXYY"},"outputs":[],"source":["token_to_id = <dictionary of symbol -> its identifier (index in tokens list)>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7umoph6eXYY"},"outputs":[],"source":["assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n","\n","for i in range(num_tokens):\n","    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n","\n","print(\"Seems alright!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PN43fWj-eXYZ"},"outputs":[],"source":["def to_matrix(names, max_len=None, pad=token_to_id[' '], dtype='int32', batch_first = True):\n","    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n","    \n","    max_len = max_len or max(map(len, names))\n","    names_ix = np.zeros([len(names), max_len], dtype) + pad\n","\n","    for i in range(len(names)):\n","        line_ix = [token_to_id[c] for c in names[i]]\n","        names_ix[i, :len(line_ix)] = line_ix\n","        \n","    if not batch_first: # convert [batch, time] into [time, batch]\n","        names_ix = np.transpose(names_ix)\n","\n","    return names_ix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQm10SwNeXYZ"},"outputs":[],"source":["#Example: cast 4 random names to matrices, pad with zeros\n","print('\\n'.join(names[::2000]))\n","print(to_matrix(names[::2000]))"]},{"cell_type":"markdown","metadata":{"id":"2I-1n06MeXYZ"},"source":["# Recurrent neural network\n","\n","We can rewrite recurrent neural network as a consecutive application of dense layer to input $x_t$ and previous rnn state $h_t$. This is exactly what we're gonna do now.\n","<img src=\"https://raw.githubusercontent.com/girafe-ai/ml-mipt/21f_made/week0_09_embeddings_and_seq2seq/rnn.png\" width=480>\n","\n","Since we're training a language model, there should also be:\n","* An embedding layer that converts character id x_t to a vector.\n","* An output layer that predicts probabilities of next phoneme"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuNMxv7JeXYa"},"outputs":[],"source":["import torch, torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYxXft8feXYa"},"outputs":[],"source":["class CharRNNCell(nn.Module):\n","    \"\"\"\n","    Implement the scheme above as torch module\n","    \"\"\"\n","    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n","        super(self.__class__,self).__init__()\n","        self.num_units = rnn_num_units\n","        \n","        self.embedding = nn.Embedding(num_tokens, embedding_size)\n","        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n","        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n","        \n","    def forward(self, x, h_prev):\n","        \"\"\"\n","        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n","        We'll call it repeatedly to produce the whole sequence.\n","        \n","        :param x: batch of character ids, containing vector of int64\n","        :param h_prev: previous rnn hidden states, containing matrix [batch, rnn_num_units] of float32\n","        \"\"\"\n","        # get vector embedding of x\n","        x_emb = self.embedding(x)\n","        \n","        # compute next hidden state using self.rnn_update\n","        # hint: use torch.cat(..., dim=...) for concatenation\n","        x_and_h = #YOUR CODE HERE\n","        h_next = #YOUR CODE HERE\n","        \n","        h_next = #YOUR CODE HERE\n","        \n","        assert h_next.size() == h_prev.size()\n","        \n","        #compute logits for next character probs\n","        logits = #YOUR CODE\n","        \n","        return h_next, F.log_softmax(logits, -1)\n","    \n","    def initial_state(self, batch_size):\n","        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n","        return torch.zeros(batch_size, self.num_units, requires_grad=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rvatNI6ceXYb"},"outputs":[],"source":["char_rnn = CharRNNCell()"]},{"cell_type":"markdown","metadata":{"id":"Hd6S3__qeXYb"},"source":["### RNN loop\n","\n","Once we've defined a single RNN step, we can apply it in a loop to get predictions on each step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCzugHxKeXYb"},"outputs":[],"source":["def rnn_loop(char_rnn, batch_ix):\n","    \"\"\"\n","    Computes log P(next_character) for all time-steps in names_ix\n","    :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n","    \"\"\"\n","    batch_size, max_length = batch_ix.size()\n","    hid_state = char_rnn.initial_state(batch_size)\n","    logprobs = []\n","\n","    for x_t in batch_ix.transpose(0,1):\n","        hid_state, logp_next = char_rnn(x_t, hid_state)  # <-- here we call your one-step code\n","        logprobs.append(logp_next)\n","        \n","    return torch.stack(logprobs, dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7gfPfEWeXYb"},"outputs":[],"source":["batch_ix = to_matrix(names[:5])\n","batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n","\n","logp_seq = rnn_loop(char_rnn, batch_ix)\n","\n","assert torch.max(logp_seq).data.numpy() <= 0\n","assert tuple(logp_seq.size()) ==  batch_ix.shape + (num_tokens,)"]},{"cell_type":"markdown","metadata":{"id":"OP6jM2FTeXYc"},"source":["### Likelihood and gradients\n","\n","We can now train our neural network to minimize crossentropy (maximize log-likelihood) with the actual next tokens.\n","\n","To do so in a vectorized manner, we take `batch_ix[:, 1:]` - a matrix of token ids shifted i step to the left so i-th element is acutally the \"next token\" for i-th prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3Le4k0VeXYc"},"outputs":[],"source":["predictions_logp = logp_seq[:, :-1]\n","actual_next_tokens = batch_ix[:, 1:]\n","\n","logp_next = torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None])\n","\n","loss = -logp_next.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXm9MV8jeXYc"},"outputs":[],"source":["loss.backward()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojxte5-geXYc"},"outputs":[],"source":["for w in char_rnn.parameters():\n","    assert w.grad is not None and torch.max(torch.abs(w.grad)).data.numpy() != 0, \\\n","        \"Loss is not differentiable w.r.t. a weight with shape %s. Check forward method.\" % (w.size(),)"]},{"cell_type":"markdown","metadata":{"id":"Ip9tRS5seXYd"},"source":["### The training loop\n","\n","We train our char-rnn exactly the same way we train any deep learning model: by minibatch sgd.\n","\n","The only difference is that this time we sample strings, not images or sound."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OBR-4O8eXYd"},"outputs":[],"source":["from IPython.display import clear_output\n","from random import sample\n","\n","char_rnn = CharRNNCell()\n","opt = torch.optim.Adam(char_rnn.parameters())\n","history = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"icXkwn-ReXYd"},"outputs":[],"source":["MAX_LENGTH = 16\n","\n","for i in range(1000):\n","    batch_ix = to_matrix(sample(names, 32), max_len=MAX_LENGTH)\n","    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n","    \n","    logp_seq = rnn_loop(char_rnn, batch_ix)\n","    \n","    # compute loss\n","    #<YOUR CODE>\n","    predictions_logp = #<YOUR CODE>\n","    actual_next_tokens = #<YOUR CODE>\n","\n","    loss = ###YOUR CODE\n","    \n","    # train with backprop\n","\n","    #<YOUR CODE>\n","    \n","    history.append(loss.data.numpy())\n","    if (i+1)%100==0:\n","        clear_output(True)\n","        plt.plot(history,label='loss')\n","        plt.legend()\n","        plt.show()\n","\n","assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""]},{"cell_type":"markdown","metadata":{"id":"SIBHUh9KeXYd"},"source":["### RNN: sampling\n","Once we've trained our network a bit, let's get to actually generating stuff. \n","All we need is the single rnn step function you have defined in `char_rnn.forward`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvzKv6i9eXYd"},"outputs":[],"source":["def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n","    '''\n","    The function generates text given a phrase of length at least SEQ_LENGTH.\n","    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n","    :param max_length: maximum output length, including seed_phrase\n","    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n","                        smaller temperature converges to the single most likely output\n","    '''\n","    \n","    x_sequence = [token_to_id[token] for token in seed_phrase]\n","    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n","    hid_state = char_rnn.initial_state(batch_size=1)\n","    \n","    #feed the seed phrase, if any\n","    for i in range(len(seed_phrase) - 1):\n","        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n","    \n","    #start generating\n","    for _ in range(max_length - len(seed_phrase)):\n","        hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)\n","        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n","        \n","        # sample next token and push it back into x_sequence\n","        next_ix = np.random.choice(num_tokens,p=p_next)\n","        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n","        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n","        \n","    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dmBfgy2veXYe"},"outputs":[],"source":["for _ in range(10):\n","    print(generate_sample(char_rnn))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Ji8W8TXeXYe"},"outputs":[],"source":["for _ in range(50):\n","    print(generate_sample(char_rnn, seed_phrase=' Sherl'))"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"-bGU5uHleXYe"},"source":["### More seriously\n","\n","What we just did is a manual low-level implementation of RNN. While it's cool, i guess you won't like the idea of re-writing it from scratch on every occasion. \n","\n","As you might have guessed, torch has a solution for this. To be more specific, there are two options:\n","* `nn.RNNCell(emb_size, rnn_num_units)` - implements a single step of RNN just like you did. Basically concat-linear-tanh\n","* `nn.RNN(emb_size, rnn_num_units` - implements the whole rnn_loop for you.\n","\n","There's also `nn.LSTMCell` vs `nn.LSTM`, `nn.GRUCell` vs `nn.GRU`, etc. etc.\n","\n","In this example we'll rewrite the char_rnn and rnn_loop using high-level rnn API."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qg6wYgR6eXYe"},"outputs":[],"source":["class CharRNNLoop(nn.Module):\n","    def __init__(self, num_tokens=num_tokens, emb_size=16, rnn_num_units=64):\n","        super(self.__class__, self).__init__()\n","        self.emb = nn.Embedding(num_tokens, emb_size)\n","        self.rnn = nn.RNN(emb_size, rnn_num_units, batch_first=True)\n","        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n","        \n","    def forward(self, x):\n","        assert isinstance(x.data, torch.LongTensor)\n","        h_seq, _ = self.rnn(self.emb(x))\n","        next_logits = self.hid_to_logits(h_seq)\n","        next_logp = F.log_softmax(next_logits, dim=-1)\n","        return next_logp\n","    \n","model = CharRNNLoop()\n","opt = torch.optim.Adam(model.parameters())\n","history = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DaJvGndeXYe"},"outputs":[],"source":["# the model applies over the whole sequence\n","batch_ix = to_matrix(sample(names, 32), max_len=MAX_LENGTH)\n","batch_ix = torch.LongTensor(batch_ix)\n","\n","logp_seq = model(batch_ix)\n","\n","# compute loss. \n","loss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n","                  batch_ix[:, :-1].contiguous().view(-1))\n","\n","loss.backward()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AsB3Lot2eXYf"},"outputs":[],"source":["MAX_LENGTH = 16\n","\n","for i in range(1000):\n","    batch_ix = to_matrix(sample(names, 32), max_len=MAX_LENGTH)\n","    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n","    \n","    logp_seq = model(batch_ix)\n","    \n","    # compute loss\n","    #<YOUR CODE>\n","    predictions_logp = #<YOUR CODE>\n","    actual_next_tokens = #<YOUR CODE>\n","\n","    loss = ###YOUR CODE\n","    \n","    # train with backprop\n","\n","    #<YOUR CODE>\n","    \n","    history.append(loss.data.numpy())\n","    if (i+1)%100==0:\n","        clear_output(True)\n","        plt.plot(history,label='loss')\n","        plt.legend()\n","        plt.show()\n","\n","assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""]},{"cell_type":"markdown","metadata":{"id":"7w7Rq0odeXYf"},"source":["### To sum up:\n","- PyTorch is convenient both for prototyping and production\n","- There are a lot of pre-implemented methods/layers/activations out of the box\n","- It's much easier (*really easier*) to use PyTorch than TensorFlow on entry level. \n","- Neural networks are not *black boxes*, they are pretty nice and easy to use (almost always)."]},{"cell_type":"markdown","metadata":{"id":"6N4UCU0QeXYf"},"source":["### Try it out!\n","You've just implemented a recurrent language model that can be tasked with generating any kind of sequence, so there's plenty of data you can try it on:\n","\n","* Novels/poems/songs of your favorite author\n","* News titles/clickbait titles\n","* Source code of Linux or Tensorflow\n","* Molecules in [smiles](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) format\n","* Melody in notes/chords format\n","* Ikea catalog titles\n","* Pokemon names\n","* Cards from Magic, the Gathering / Hearthstone\n","\n","If you're willing to give it a try, here's what you wanna look at:\n","* Current data format is a sequence of lines, so a novel can be formatted as a list of sentences. Alternatively, you can change data preprocessing altogether.\n","* While some datasets are readily available, others can only be scraped from the web. Try `Selenium` or `Scrapy` for that.\n","* Make sure MAX_LENGTH is adjusted for longer datasets. There's also a bonus section about dynamic RNNs at the bottom.\n","* More complex tasks require larger RNN architecture, try more neurons or several layers. It would also require more training iterations.\n","* Long-term dependencies in music, novels or molecules are better handled with LSTM or GRU\n","\n","__Good hunting!__"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"week09_seq2seq_rnn_practice.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}